{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# !pip download transformers==4.16.2\n",
    "# !pip download tokenizers==0.11.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries\n",
    "\"\"\"\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedTokenizerFast\n",
    "from transformers.models.deberta.tokenization_deberta_fast import DebertaTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from IPython.core.display_functions import display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tokenizer\n",
    "\"\"\"\n",
    "# transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "input_dir = Path(\"kaggle/input/tokenizers/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "transformers_path = Path(\"/home/noone/anaconda3/envs/NBME/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "\n",
    "# CONFIGURATION.tokenizer = PreTrainedTokenizerFast('kaggle/input/checkpoints/deberta-large/tokenizer')\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(BPE())\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "# tokenizer.train(files=[\"wiki.train.raw\", \"wiki.valid.raw\", \"wiki.test.raw\"], trainer=trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kaggle/input/models/deberta_large_model were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CONFIGURATION:\n",
    "    num_workers = 4\n",
    "    path = \"kaggle/input/checkpoints/deberta-large/\"  # todo: adjust this for each scoring attempt\n",
    "    config_path = path + 'config.pth'\n",
    "    model = AutoModel.from_pretrained(\"kaggle/input/models/deberta_large_model\")\n",
    "    batch_size = 24\n",
    "    fc_dropout = 0.2\n",
    "    max_len = 466\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.10.3\n",
      "transformers.__version__: 4.14.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "settings\n",
    "\"\"\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "TOKENIZERS_PARALLELISM = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "tokenizer_file = 'kaggle/input/checkpoints/deberta-large/tokenizer/tokenizer.json'\n",
    "\n",
    "tokenizer = DebertaTokenizerFast(vocab_file=None,\n",
    "                     marges_file=None,\n",
    "                     tokenizer_file=tokenizer_file,\n",
    "                     bos_token='[CLS]',\n",
    "                     eos_token='[SEP]',\n",
    "                     cls_token='[CLS]',\n",
    "                     unk_token='[UNK]',\n",
    "                     pad_token='[PAD]',\n",
    "                     mask_token='[MASK]',\n",
    "                     add_prefix_space=False)\n",
    "\n",
    "# tokenizer = DebertaTokenizerFast.from_pretrained('kaggle/input/tokenizers/deberta-tokenizer')\n",
    "CONFIGURATION.tokenizer = tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "helper functions for scoring\n",
    "\"\"\"\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "    Args: spans (list of lists of two ints): Spans.\n",
    "    Returns: np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "    preds: list of lists of two ints, prediction spans.\n",
    "    truths: list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns: float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utils\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename='inference'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.45  score: 0.37183\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.46  score: 0.37170\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.47  score: 0.37164\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.48  score: 0.37138\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.49  score: 0.37128\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.5  score: 0.37120\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.51  score: 0.37113\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.52  score: 0.37090\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.53  score: 0.37088\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.54  score: 0.37079\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "th: 0.55  score: 0.37062\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n",
      "best_th: 0.45  score: 0.37183\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "oof\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "oof = pd.read_pickle('kaggle/input/checkpoints/deberta-large/oof_df.pkl')\n",
    "\n",
    "truths = create_labels_for_scoring(oof)\n",
    "char_probs = get_char_probs(oof['pn_history'].values,\n",
    "                            oof[[i for i in range(CONFIGURATION.max_len)]].values,\n",
    "                            CONFIGURATION.tokenizer)\n",
    "best_th = 0.5\n",
    "best_score = 0.\n",
    "for th in np.arange(0.45, 0.55, 0.01):\n",
    "    th = np.round(th, 2)\n",
    "    results = get_results(char_probs, th=th)\n",
    "    preds = get_predictions(results)\n",
    "    score = get_score(preds, truths)\n",
    "    if best_score < score:\n",
    "        best_th = th\n",
    "        best_score = score\n",
    "    LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n",
    "LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (5, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          id  case_num  pn_num  feature_num\n0  00016_000         0      16            0\n1  00016_001         0      16            1\n2  00016_002         0      16            2\n3  00016_003         0      16            3\n4  00016_004         0      16            4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.shape: (143, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   feature_num  case_num                                       feature_text\n0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n1            1         0                 Family-history-of-thyroid-disorder\n2            2         0                                     Chest-pressure\n3            3         0                              Intermittent-symptoms\n4            4         0                                        Lightheaded",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_num</th>\n      <th>case_num</th>\n      <th>feature_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>Family-history-of-thyroid-disorder</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>Chest-pressure</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>Intermittent-symptoms</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>Lightheaded</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_notes.shape: (42146, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          id  case_num  pn_num  feature_num\n0  00016_000         0      16            0\n1  00016_001         0      16            1\n2  00016_002         0      16            2\n3  00016_003         0      16            3\n4  00016_004         0      16            4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "          id  case_num  pn_num  feature_num                                       feature_text                                         pn_history\n0  00016_000         0      16            0  Family-history-of-MI-OR-Family-history-of-myoc...  HPI: 17yo M presents with palpitations. Patien...\n1  00016_001         0      16            1                 Family-history-of-thyroid-disorder  HPI: 17yo M presents with palpitations. Patien...\n2  00016_002         0      16            2                                     Chest-pressure  HPI: 17yo M presents with palpitations. Patien...\n3  00016_003         0      16            3                              Intermittent-symptoms  HPI: 17yo M presents with palpitations. Patien...\n4  00016_004         0      16            4                                        Lightheaded  HPI: 17yo M presents with palpitations. Patien...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case_num</th>\n      <th>pn_num</th>\n      <th>feature_num</th>\n      <th>feature_text</th>\n      <th>pn_history</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00016_000</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00016_001</td>\n      <td>0</td>\n      <td>16</td>\n      <td>1</td>\n      <td>Family-history-of-thyroid-disorder</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00016_002</td>\n      <td>0</td>\n      <td>16</td>\n      <td>2</td>\n      <td>Chest-pressure</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00016_003</td>\n      <td>0</td>\n      <td>16</td>\n      <td>3</td>\n      <td>Intermittent-symptoms</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00016_004</td>\n      <td>0</td>\n      <td>16</td>\n      <td>4</td>\n      <td>Lightheaded</td>\n      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Loading\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "test = pd.read_csv('kaggle/input/nbme-score-clinical-patient-notes/test.csv')\n",
    "submission = pd.read_csv('kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv')\n",
    "features = pd.read_csv('kaggle/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "\n",
    "\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv('kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "display(test.head())\n",
    "test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "display(test.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CONFIGURATION.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg,\n",
    "                               self.pn_historys[item],\n",
    "                               self.feature_texts[item])\n",
    "        return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DebertaModel' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [136]\u001B[0m, in \u001B[0;36m<cell line: 28>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold \u001B[38;5;129;01min\u001B[39;00m CONFIGURATION\u001B[38;5;241m.\u001B[39mtrn_fold:\n\u001B[1;32m     29\u001B[0m     model \u001B[38;5;241m=\u001B[39m CustomModel(CONFIGURATION, config_path\u001B[38;5;241m=\u001B[39mCONFIGURATION\u001B[38;5;241m.\u001B[39mconfig_path, pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m     state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(CONFIGURATION\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIGURATION\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_fold\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_best.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m, map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     31\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     32\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m inference_fn(test_loader, model, device)\n",
      "File \u001B[0;32m~/anaconda3/envs/NBME/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1185\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DebertaModel' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "inference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "test_dataset = TestDataset(CONFIGURATION, test)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=CONFIGURATION.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=CONFIGURATION.num_workers, pin_memory=True, drop_last=False)\n",
    "predictions = []\n",
    "\n",
    "for fold in CONFIGURATION.trn_fold:\n",
    "    model = CustomModel(CONFIGURATION, config_path=CONFIGURATION.config_path, pretrained=False)\n",
    "    state = torch.load(CONFIGURATION.path + f\"{CONFIGURATION.model.split('/')[1]}_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    prediction = prediction.reshape((len(test), CONFIGURATION.max_len))\n",
    "    char_probs = get_char_probs(test['pn_history'].values, prediction, CONFIGURATION.tokenizer)\n",
    "    predictions.append(char_probs)\n",
    "    del model, state, prediction, char_probs; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "predictions = np.mean(predictions, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "submission\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "results = get_results(predictions, th=best_th)\n",
    "submission['location'] = results\n",
    "display(submission.head())\n",
    "submission[['id', 'location']].to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}